# -*- coding: utf-8 -*-
"""NLP_Neural_Language_Modeling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GlmSlyINjMCBnk67OQo201ha6cb-Avf1

# **Neural Language Modeling**


1.   **Library** (nltk, torch)
2.   **Result** (Word prediction using Neural Networks)
"""

import io
import re
from nltk import FreqDist
from nltk.util import ngrams
import collections
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

"""Downlaod data"""

!gdown --id 1qCVYpb67RuzUbyrJ3w-ohtCf-tzZKTal
!gdown --id 1dW5SkCYIFbXmNe3xKv4EhrLPDPlXyIDy

"""Preprocess

> Ignoring

*   extra consecutive spaces
*   spaces at the beginning and end of lines
"""

def file_clear(in_f, out_f):
    inputFile = io.open(in_f, mode="r", encoding="utf-8")
    outputFile = io.open(out_f, mode="w", encoding="utf-8")

    updated=[]
    Lines = inputFile.readlines()
    for i, line in enumerate(Lines):
        newline= re.sub(' +', ' ', line.strip())
        if len(line.split()) >2:
            updated.append(newline+"\n")
        else:
            print(line,i)
    outputFile.writelines(updated)
    print( len(Lines), len(updated))
    inputFile.__exit__()
    outputFile.__exit__()
#updated

file_clear('train.txt', 'cleaned_train.txt')
file_clear('test.txt', 'cleaned_test.txt')

"""Q. Complete the following expressions using the neural language modeling(trigram)


1.   چون مشک سیه بود مرا هر دو بنا
2.   گر خورد سوگند هم آن
3.   زانک نفس آشفته تر گردد از
4.   ازین زشت تر در جهان رنگ
5.   دوست در خانه و ما گرد
6.   شب است و شمع و شراب و


"""

#Remove Big Prev Model
lm = None

#Get Words
def compute_freq(inputfile, nu):
    textfile = io.open(inputfile, mode="r", encoding="utf-8")
    Lines = textfile.readlines()
    textfile.__exit__()
    ngramfdist = FreqDist()

    for i,line in enumerate(Lines):
        tokens = line.strip().split(' ')
        if len(tokens)>nu:
            # ngram_ = ngrams(tokens, nu)
            # print(i+1," ",line)
            ngram_ = ngrams(tokens, nu)
            ngramfdist.update(ngram_)
    return ngramfdist

words = compute_freq("cleaned_train.txt",1)
#words_dict = collections.Counter(ngrams)
words = [i[0] for i in words]
word2int = dict(enumerate(words))
word2int = {word: ind for ind, word in word2int.items()}
int2word = {value: key for key, value in word2int.items()}
int2word.keys()

vocab_size= len(int2word)

#Get Trigrams
def generateSeq(inputfile, nu):
    textfile = io.open(inputfile, mode="r", encoding="utf-8")
    Lines = textfile.readlines()
    textfile.__exit__()
    sequences={}
    seqIndex=0
    for i,line in enumerate(Lines):
        tokens = line.strip().split(' ')
        if len(tokens)>nu:
            ngrams_ = ngrams(tokens, nu)
            for tuple_ in ngrams_:
                t = list(tuple_)
                sequences[seqIndex]= ([word2int[i] for i in t[:-1]], word2int[t[-1]] )
                seqIndex = seqIndex+1
    return sequences

sequences_dict = generateSeq("cleaned_train.txt",3)

CONTEXT_SIZE = 2
EMBEDDING_DIM = 128

class Dataset(torch.utils.data.Dataset):
  'Characterizes a dataset for PyTorch'
  def __init__(self, sequence_dict):
        'Initialization'
        self.data_dict = sequence_dict
        # self.labels = labels
        # self.list_IDs = list_IDs

  def __len__(self):
        'Denotes the total number of samples'
        return len(self.data_dict)

  def __getitem__(self, index):
        'Generates one sample of data'
        # Select sample
        # ID = self.list_IDs[index]

        # # Load data and get label
        # X = torch.load('data/' + ID + '.pt')
        # y = self.labels[ID]
        x,y = self.data_dict[index]
        return tuple(x), y

class NGramLanguageModeler(nn.Module):

    def __init__(self, vocab_size, embedding_dim, context_size):
        super(NGramLanguageModeler, self).__init__()
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.linear1 = nn.Linear(context_size * embedding_dim, 128)
        self.linear2 = nn.Linear(128, vocab_size)

    def forward(self, inputs):
        #embeds = self.embeddings(inputs).view((1, -1))
        embeds= torch.cat((self.embeddings(inputs[0]),self.embeddings(inputs[1])),1)
        out = F.relu(self.linear1(embeds))
        out = self.linear2(out)
        log_probs = F.log_softmax(out, dim=1)
        return log_probs

# Parameters
params = {'batch_size': 1024*8,
          'shuffle': True,
          'num_workers': 0}
max_epochs = 100

# Generators
training_set = Dataset(sequences_dict)
training_generator = torch.utils.data.DataLoader(training_set, **params)


use_cuda = torch.cuda.is_available()
device = torch.device("cuda:0" if use_cuda else "cpu")
torch.backends.cudnn.benchmark = True

losses = []
loss_function = nn.NLLLoss()
model = NGramLanguageModeler(vocab_size, EMBEDDING_DIM, CONTEXT_SIZE).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

model.train()
for epoch in range(max_epochs):
    total_loss = 0
    for context, target in training_generator:

        context_idxs = [context[0].to(device), context[1].to(device)]
        model.zero_grad()

        log_probs = model(context_idxs)

        loss = loss_function(log_probs, target.to(device))

        loss.backward()
        optimizer.step()

        total_loss += loss.item()
    print(epoch, " ",total_loss)
    losses.append(total_loss)
print(losses)  # The loss decreased every iteration over the training data!

# To get the embedding of a particular word, e.g. "beauty"

sents_to_comp=[ ("چون مشک سیه بود مرا هر دو", 2),
                ("گر خورد سوگند هم آن", 1),
                ("زانک نفس آشفته تر گردد از",1),
                ("ازین زشت تر در جهان رنگ",1),
                ("دوست در خانه و ما گرد",2),
                ("شب است و شمع و شراب و",1)
]

model.eval()
for sent, i2 in  sents_to_comp:
    for i in range(i2):
        sent_words= sent.strip().split()
        context = [ torch.tensor([word2int[sent_words[-2]]]).to(device) , torch.tensor([word2int[sent_words[-1]]]).to(device)]
        nextWordPreds = model(context)
        nextWordID = np.argmax(nextWordPreds.detach().cpu()).item()
        nextWord = int2word[nextWordID]
        sent = sent + " "+ nextWord
    print(sent)